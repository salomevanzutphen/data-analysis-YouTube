{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYz1qiyOaWcR"
      },
      "source": [
        "**1.0 Data Preparation (non-rerunnable, privacy-restriced)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMQRYRLJp6zT"
      },
      "source": [
        "The comments have been extracted externally and uploaded into a dataset, accessible via google drive. This part reviews the data and prepares it by removing any deleted comments, adding a stable index and anonymising usernames for privacy reasons. The stable index ensures consistency during the thematic coding, especially considering the dataset may be reloaded which causes default indexes to change.\n",
        "\n",
        "For privacy reasons, this part of the data analysis is not rerunable in Github, as the file contains the original authornames. For rerunning the code, please start from step **1.1 GitHub-Ready Code**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D0FLebWGaKmI"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UtMfUfdxlMt4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "file_path = \"/content/drive/MyDrive/Dataprojects/Methodology/videoinfo_SdSSPF1S-Uc&t=24s_2025_10_16-23_09_59_comments.csv\"\n",
        "df_new = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"‚úÖ Loaded {len(df_new):,} rows from {file_path}\")\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "df_new['text'].head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Dataprojects/Methodology/videoinfo_SdSSPF1S-Uc&t=24s_2025_10_16-23_09_59_comments.csv\"\n",
        "df_raw = pd.read_csv(file_path)\n",
        "\n",
        "print(f\"üì• Loaded {len(df_raw):,} rows\")\n",
        "\n",
        "text_col = None\n",
        "for c in [\"text\", \"comment\", \"body\", \"content\"]:\n",
        "    if c in df_raw.columns:\n",
        "        text_col = c\n",
        "        break\n",
        "\n",
        "if text_col is None:\n",
        "    raise ValueError(\"‚ùå No comment text column found\")\n",
        "\n",
        "print(f\"üìù Using text column: {text_col}\")\n",
        "\n",
        "df = df_raw.copy()\n",
        "\n",
        "mask_blank = (\n",
        "    df[text_col].isna() |\n",
        "    df[text_col].astype(str).str.strip().eq(\"\")\n",
        ")\n",
        "\n",
        "num_blank = mask_blank.sum()\n",
        "num_total = len(df)\n",
        "\n",
        "print(f\"\\nüóëÔ∏è Blank / unavailable comments detected: {num_blank:,}\")\n",
        "print(f\"üìä Share of dataset removed: {num_blank / num_total * 100:.2f}%\")\n",
        "\n",
        "df_clean = df[~mask_blank].copy()\n",
        "\n",
        "df_clean = df_clean.reset_index(drop=True)\n",
        "df_clean[\"stable_comment_index\"] = df_clean.index\n",
        "\n",
        "print(f\"‚ú® Cleaned dataset size: {len(df_clean):,}\")\n",
        "\n",
        "USERNAME_COL = \"authorName\"\n",
        "\n",
        "if USERNAME_COL not in df_clean.columns:\n",
        "    raise ValueError(f\"‚ùå Username column '{USERNAME_COL}' not found\")\n",
        "\n",
        "unique_authors = df_clean[USERNAME_COL].dropna().unique()\n",
        "author_to_anon = {author: f\"User_{i+1}\" for i, author in enumerate(unique_authors)}\n",
        "\n",
        "df_clean[\"user_anonymised\"] = df_clean[USERNAME_COL].map(author_to_anon)\n",
        "df_clean[\"user_anonymised\"] = df_clean[\"user_anonymised\"].fillna(\"user_deleted\")\n",
        "\n",
        "print(f\"üë§ Created 'user_anonymised' for {len(unique_authors):,} users.\")\n",
        "\n",
        "df_clean.drop(columns=[USERNAME_COL], inplace=True)\n",
        "print(f\"üîí Dropped identifying column: '{USERNAME_COL}'\")\n",
        "\n",
        "final_output_path = \"/content/drive/MyDrive/Dataprojects/Methodology/youtube_final_dataset.csv\"\n",
        "df_clean.to_csv(final_output_path, index=False)\n",
        "\n",
        "print(f\"\\nüíæ Final dataset saved to:\")\n",
        "print(final_output_path)\n",
        "print(f\"üìå Total final comments: {len(df_clean):,}\")\n"
      ],
      "metadata": {
        "id": "ooSet3IDrd2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 GitHub-Ready Code**\n"
      ],
      "metadata": {
        "id": "67OcQ_ct4sHq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "CANDIDATE_PATHS = [\n",
        "    Path(\"data/raw/youtube_final_dataset.csv\"),\n",
        "    Path(\"../data/raw/youtube_final_dataset.csv\"),\n",
        "]\n",
        "\n",
        "CLEAN_YOUTUBE_DATA = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
        "if CLEAN_YOUTUBE_DATA is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find youtube_final_dataset.csv. Expected it in data/raw/.\\n\"\n",
        "        \"Make sure you've cloned the repo and the file exists at data/raw/youtube_final_dataset.csv\"\n",
        "    )\n",
        "\n",
        "df_youtube = pd.read_csv(CLEAN_YOUTUBE_DATA)\n",
        "print(f\"Loaded {len(df_youtube):,} YouTube rows from: {CLEAN_YOUTUBE_DATA}\")\n"
      ],
      "metadata": {
        "id": "O8a9BCIk4ad3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUZsikoo1ADo"
      },
      "source": [
        "**1.2 Reviewing Duplicate Comments**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NHs6N0FWqTfG"
      },
      "source": [
        "This part includes reviewing the metrics of identical comments, to determine whether they are true or accidental duplications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3dq4nprDxObj"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "df_youtube = pd.read_csv(CLEAN_YOUTUBE_DATA)\n",
        "print(f\"Loaded {len(df_youtube):,} YouTube rows\")\n",
        "\n",
        "text_col = \"text\"\n",
        "\n",
        "df_youtube[\"publishedAt\"] = pd.to_datetime(df_youtube[\"publishedAt\"], errors=\"coerce\")\n",
        "\n",
        "duplicate_counts = df_youtube[text_col].value_counts()\n",
        "duplicate_texts = duplicate_counts[duplicate_counts > 1].index.tolist()\n",
        "\n",
        "print(f\"üîç Found {len(duplicate_texts)} duplicated unique YouTube comments.\")\n",
        "\n",
        "duplicates_full = df_youtube[df_youtube[text_col].isin(duplicate_texts)].copy()\n",
        "\n",
        "def summarize_group(g):\n",
        "    g = g.sort_values(\"publishedAt\")\n",
        "\n",
        "    freq = len(g)\n",
        "\n",
        "    rows = []\n",
        "    for _, r in g.iterrows():\n",
        "        rows.append(\n",
        "            f\"user={r.get('user_anonymised', '')}, \"\n",
        "            f\"likes={r.get('likeCount', '')}, \"\n",
        "            f\"publishedAt={r.get('publishedAt', '')}\"\n",
        "        )\n",
        "\n",
        "    instances_str = \"\\n\".join(rows)\n",
        "\n",
        "    return pd.Series({\n",
        "        \"frequency\": freq,\n",
        "        \"instances\": instances_str\n",
        "    })\n",
        "\n",
        "with warnings.catch_warnings():\n",
        "    warnings.simplefilter(\"ignore\")\n",
        "    summary_youtube = duplicates_full.groupby(text_col).apply(summarize_group).reset_index()\n",
        "\n",
        "summary_youtube = summary_youtube.sort_values(\"frequency\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "summary_youtube.index = summary_youtube.index + 1\n",
        "\n",
        "# Display table\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "\n",
        "display(\n",
        "    summary_youtube.style.set_properties(\n",
        "        subset=[text_col],\n",
        "        **{\n",
        "            \"white-space\": \"normal\",\n",
        "            \"max-width\": \"300px\",\n",
        "        }\n",
        "    ).set_properties(\n",
        "        subset=[\"instances\"],\n",
        "        **{\n",
        "            \"white-space\": \"pre-wrap\",\n",
        "            \"max-width\": \"1000px\",\n",
        "            \"font-family\": \"monospace\"\n",
        "        }\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MeGr6CYzzL7"
      },
      "source": [
        "**2.0 Opinion Leaders**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_d656H5rqro"
      },
      "source": [
        "The opinion leaders are identified using the benchmark of 75% of the total likecount/upvotes of the overall dataset. After knowing how many comments are in this subset, they are reviewed with their ranking, stable index, like count and text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PwQeajHG8ljh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(CLEAN_YOUTUBE_DATA)\n",
        "\n",
        "LIKES_COL = \"likeCount\"\n",
        "\n",
        "\n",
        "if LIKES_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{LIKES_COL}' not found. Available columns: {list(df.columns)}\")\n",
        "\n",
        "\n",
        "df = df[df[LIKES_COL].notna() & (df[LIKES_COL] >= 0)]\n",
        "\n",
        "\n",
        "total_comments = len(df)\n",
        "total_likes = df[LIKES_COL].sum()\n",
        "\n",
        "\n",
        "if total_comments == 0 or total_likes == 0:\n",
        "    print(\"No comments or no likes in the dataset.\")\n",
        "else:\n",
        "    # 75% benchmark of the total likes\n",
        "    target_likes_75 = 0.75 * total_likes\n",
        "\n",
        "    df_sorted = df.sort_values(LIKES_COL, ascending=False).reset_index(drop=True)\n",
        "    df_sorted[\"cum_likes\"] = df_sorted[LIKES_COL].cumsum()\n",
        "\n",
        "    first_row_reaching_75 = df_sorted[df_sorted[\"cum_likes\"] >= target_likes_75].index.min()\n",
        "    comments_needed = int(first_row_reaching_75) + 1\n",
        "\n",
        "    percent_of_dataset = (comments_needed / total_comments) * 100\n",
        "\n",
        "\n",
        "    print(f\"Total comments: {total_comments:,}\")\n",
        "    print(f\"Total likes: {total_likes:,}\")\n",
        "    print(f\"75% benchmark of total likes: {target_likes_75:,.0f}\")\n",
        "    print(f\"Comments needed to reach 75% of likes: {comments_needed:,}\")\n",
        "    print(f\"These comments represent {percent_of_dataset:.2f}% of the dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KH6wmD1j9IfG"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(CLEAN_YOUTUBE_DATA)\n",
        "\n",
        "LIKES_COL = \"likeCount\"\n",
        "TEXT_COL = \"text\"\n",
        "\n",
        "\n",
        "if LIKES_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{LIKES_COL}' not found. Available columns: {list(df.columns)}\")\n",
        "\n",
        "if TEXT_COL not in df.columns:\n",
        "    raise ValueError(f\"Column '{TEXT_COL}' not found. Available columns: {list(df.columns)}\")\n",
        "\n",
        "\n",
        "opinionleaders_youtube = (\n",
        "    df.sort_values(LIKES_COL, ascending=False)\n",
        "      .head(35)\n",
        "      .copy()\n",
        ")\n",
        "\n",
        "# Preserve original index\n",
        "opinionleaders_youtube[\"index\"] = opinionleaders_youtube.index\n",
        "\n",
        "\n",
        "opinionleaders_youtube.index = range(1, len(opinionleaders_youtube) + 1)\n",
        "opinionleaders_youtube.index.name = None\n",
        "\n",
        "\n",
        "opinionleaders_youtube = opinionleaders_youtube[[LIKES_COL, \"index\", TEXT_COL]]\n",
        "\n",
        "\n",
        "styled_table_youtube = opinionleaders_youtube.style.set_properties(\n",
        "    subset=[TEXT_COL],\n",
        "    **{\n",
        "        \"white-space\": \"normal\",\n",
        "        \"word-wrap\": \"break-word\",\n",
        "        \"width\": \"550px\"\n",
        "    }\n",
        ")\n",
        "\n",
        "styled_table_youtube\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JL_SVpU2yNi"
      },
      "source": [
        "**2.1 Thematic Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vB0vE0IL_xnw"
      },
      "source": [
        "This part categorizes each comment within the subset of opinion leaders. The following themes have been identified:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAApr5rkCDQX"
      },
      "source": [
        "-  **Nostalgia & Comeback Angels**: Appreciation for the original models and nostalgic elements of the traditional VS shows\n",
        "- **VS Show Production**: Regarding the overall experience of watching the show & highlighting certain segments or specific elements\n",
        "-   **Inclusivity**: Positive feedback on a variety of cultures, sizes, races, etc.\n",
        "-   **Music Artists**: Regarding the performances and artists\n",
        "-   **Sisterhood**: Highlighting discourse surrounding women's empowerment and the collective identity of girls\n",
        "-   **Year on Year**: Highlighting this year's improvement & comparisons to last year\n",
        "-   **Specific Model References**: Specifically about particular models and their stage performance\n",
        "-   **Timestamps**: Listing times of performances, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgVgY0Lwz9Lv"
      },
      "source": [
        "Below follows the thematic distribution of comments and likecounts, visualised in a pie chart. Furthermore, all comments and their related themes are be provided as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIqvJ7oWBW9y"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "df_clean = pd.read_csv(CLEAN_YOUTUBE_DATA)\n",
        "print(f\"Loaded YouTube dataset with {len(df_clean):,} rows.\")\n",
        "\n",
        "themes_by_index = {\n",
        "    \"Nostalgia & Comeback Angels\": [9411, 9490, 10402, 12158, 15288, 6338, 19051, 12483, 17573],\n",
        "    \"Inclusivity\": [13333, 7152, 8481],\n",
        "    \"VS ShowProduction\": [19552, 14457, 13333, 15288, 2817, 15498, 5575],\n",
        "    \"Music Artists\": [10447, 7549, 12158, 18322, 19023, 3051, 16332, 15498, 11539, 4070, 17573],\n",
        "    \"Timestamps\": [16694, 3538],\n",
        "    \"Sisterhood\": [17673, 19051, 17901, 7152],\n",
        "    \"Year on Year\": [13333, 19318, 16141, 4070, 14037],\n",
        "    \"Specific Model References\": [18671, 14457, 8125, 19278],\n",
        "}\n",
        "\n",
        "candidate_cols = []\n",
        "for c in df_clean.columns:\n",
        "    cl = c.lower()\n",
        "    if cl in [\"text\", \"content\", \"body\", \"comment\", \"likecount\", \"score\"]:\n",
        "        candidate_cols.append(c)\n",
        "\n",
        "if not candidate_cols:\n",
        "    raise ValueError(\n",
        "        \"No usable columns found. Expected one of: text/content/body/comment and likeCount/score.\"\n",
        "    )\n",
        "\n",
        "subset = df_clean[candidate_cols].copy()\n",
        "\n",
        "text_col = None\n",
        "for c in subset.columns:\n",
        "    if c.lower() in [\"text\", \"content\", \"body\", \"comment\"]:\n",
        "        text_col = c\n",
        "        break\n",
        "if text_col is None:\n",
        "    raise ValueError(\"No text column found (expected 'text', 'content', 'body', or 'comment').\")\n",
        "\n",
        "likes_col = None\n",
        "for c in subset.columns:\n",
        "    if c.lower() in [\"likecount\", \"score\"]:\n",
        "        likes_col = c\n",
        "        break\n",
        "if likes_col is None:\n",
        "    raise ValueError(\"No likes column found (expected 'likeCount' or 'score').\")\n",
        "\n",
        "subset = subset.rename(columns={text_col: \"comment\", likes_col: \"likeCount\"})\n",
        "\n",
        "subset[\"likeCount\"] = subset[\"likeCount\"].fillna(0).astype(int)\n",
        "subset[\"comment\"] = subset[\"comment\"].astype(str).fillna(\"\").str.strip()\n",
        "\n",
        "top_comments_df = (\n",
        "    subset.sort_values(\"likeCount\", ascending=False)\n",
        "          .head(35)[[\"comment\", \"likeCount\"]]\n",
        "          .copy()\n",
        ")\n",
        "\n",
        "ranked = top_comments_df.reset_index().rename(columns={\"index\": \"orig_index\"})\n",
        "\n",
        "\n",
        "index_to_themes = {}\n",
        "for theme, indices in themes_by_index.items():\n",
        "    for idx in indices:\n",
        "        index_to_themes.setdefault(idx, []).append(theme)\n",
        "\n",
        "def join_labels(idx):\n",
        "    labels = index_to_themes.get(idx, [])\n",
        "    return \", \".join(labels) if labels else \"\"\n",
        "\n",
        "ranked[\"themes\"] = ranked[\"orig_index\"].apply(join_labels)\n",
        "\n",
        "missing = ranked.loc[ranked[\"themes\"].eq(\"\"), \"orig_index\"].tolist()\n",
        "if missing:\n",
        "    print(f\"‚ö†Ô∏è Dataset indices in top-35 with no theme label: {missing}\")\n",
        "else:\n",
        "    print(\"‚úÖ All top-35 comments have at least one theme label.\")\n",
        "\n",
        "ranked[\"theme_list\"] = ranked[\"themes\"].apply(\n",
        "    lambda x: x.split(\", \") if isinstance(x, str) and x else []\n",
        ")\n",
        "\n",
        "long = ranked.explode(\"theme_list\").rename(columns={\"theme_list\": \"theme\"})\n",
        "long = long[long[\"theme\"] != \"\"]\n",
        "\n",
        "\n",
        "counts = long.groupby(\"theme\")[\"orig_index\"].nunique().sort_values(ascending=False)\n",
        "likes = long.groupby(\"theme\")[\"likeCount\"].sum().sort_values(ascending=False)\n",
        "\n",
        "total_likes = likes.sum()\n",
        "\n",
        "summary_df = pd.DataFrame({\n",
        "    \"Comments\": counts,\n",
        "    \"Likes\": likes,\n",
        "    \"Share of Likes (%)\": (likes / total_likes * 100).round(1),\n",
        "}).sort_values(\"Likes\", ascending=False)\n",
        "\n",
        "print(\"\\nüìä THEME SUMMARY (Top 35 YouTube Comments)\\n\")\n",
        "print(summary_df.to_string())\n",
        "\n",
        "# --- Plot ---\n",
        "plt.figure(figsize=(10, 7))\n",
        "\n",
        "wedges, texts, autotexts = plt.pie(\n",
        "    summary_df[\"Likes\"],\n",
        "    labels=None,\n",
        "    autopct=\"%1.1f%%\",\n",
        "    startangle=140,\n",
        "    pctdistance=0.75\n",
        ")\n",
        "\n",
        "plt.title(\"Thematic Engagement Distribution\")\n",
        "\n",
        "plt.legend(\n",
        "    wedges,\n",
        "    summary_df.index,\n",
        "    title=\"Theme\",\n",
        "    loc=\"center left\",\n",
        "    bbox_to_anchor=(1.02, 0.5),\n",
        "    frameon=False\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# --- GitHub-ready output path (stable) ---\n",
        "OUTPUT_DIR = Path(\"data/processed\")\n",
        "if not OUTPUT_DIR.exists():\n",
        "    alt = Path(\"../data/processed\")\n",
        "    if alt.parent.exists():\n",
        "        OUTPUT_DIR = alt\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "OUT_PATH = OUTPUT_DIR / \"top35_youtube_comments_labeled_by_theme.csv\"\n",
        "ranked.to_csv(OUT_PATH, index=False, encoding=\"utf-8\")\n",
        "print(f\"\\nüìÅ Saved labeled top-35 to: {OUT_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4S1aBcAlBajR"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import textwrap\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Find the labeled file in common locations ---\n",
        "CANDIDATE_PATHS = [\n",
        "    Path(\"data/processed/top35_youtube_comments_labeled_by_theme.csv\"),\n",
        "    Path(\"../data/processed/top35_youtube_comments_labeled_by_theme.csv\"),\n",
        "]\n",
        "\n",
        "LABELED_PATH = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
        "if LABELED_PATH is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find the labeled CSV. Run the previous code block first to generate it.\\n\"\n",
        "        \"Expected: data/processed/top35_youtube_comments_labeled_by_theme.csv\"\n",
        "    )\n",
        "\n",
        "df_labeled_comments = pd.read_csv(LABELED_PATH)\n",
        "print(f\"‚úÖ Loaded labeled comments from: {LABELED_PATH}\")\n",
        "\n",
        "if \"index\" in df_labeled_comments.columns:\n",
        "    df_labeled_comments = df_labeled_comments.rename(columns={\"index\": \"orig_index\"})\n",
        "\n",
        "df_labeled_comments[\"themes\"] = df_labeled_comments[\"themes\"].apply(\n",
        "    lambda x: [theme.strip() for theme in x.split(\",\")] if pd.notna(x) and x else []\n",
        ")\n",
        "\n",
        "all_unique_themes = sorted(\n",
        "    {theme for sublist in df_labeled_comments[\"themes\"] for theme in sublist}\n",
        ")\n",
        "\n",
        "MAX_WIDTH = 100\n",
        "\n",
        "print(\"\\n=== YouTube Comments by Theme (Top 35) ===\\n\")\n",
        "\n",
        "for theme in all_unique_themes:\n",
        "    print(f\"\\n--- üîµ Theme: {theme} ---\")\n",
        "\n",
        "    theme_comments = df_labeled_comments[\n",
        "        df_labeled_comments[\"themes\"].apply(lambda x: theme in x)\n",
        "    ]\n",
        "\n",
        "    if theme_comments.empty:\n",
        "        print(f\"  No comments found for '{theme}'.\")\n",
        "        continue\n",
        "\n",
        "    theme_comments = theme_comments.sort_values(by=\"likeCount\", ascending=False)\n",
        "\n",
        "    for i, row in enumerate(theme_comments.itertuples(index=False), start=1):\n",
        "        wrapped_comment = textwrap.fill(\n",
        "            row.comment,\n",
        "            width=MAX_WIDTH,\n",
        "            break_long_words=False,\n",
        "            replace_whitespace=False,\n",
        "        )\n",
        "        print(f\"  {i}.  {row.likeCount} likes  |  Original Index: {row.orig_index}\")\n",
        "        print(f\"     {wrapped_comment}\")\n",
        "        print(\"-\" * MAX_WIDTH)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mVdBNJJ2kcF9"
      },
      "source": [
        "**3.0 Sentiment Analysis**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqwbudu-xTU-"
      },
      "source": [
        "This includes a sentiment analysis on the overall dataset, as well as a manual review of 100 comments. Here, the doubtful or incorrect classifications are separated for closer review, before calculating the accuracy rate.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dx6W06hODxJ"
      },
      "outputs": [],
      "source": [
        "!pip install -q transformers torch\n",
        "\n",
        "import pandas as pd\n",
        "import torch\n",
        "import time\n",
        "from pathlib import Path\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "df = pd.read_csv(CLEAN_YOUTUBE_DATA)\n",
        "print(f\"‚úÖ Loaded {len(df):,} YouTube rows from:\\n{CLEAN_YOUTUBE_DATA}\")\n",
        "\n",
        "# --- GitHub-ready output path (stable) ---\n",
        "# Default assumes running from repo root. If running from /notebooks, fall back to ../data/processed.\n",
        "OUTPUT_DIR = Path(\"data/processed\")\n",
        "if not OUTPUT_DIR.exists():\n",
        "    alt = Path(\"../data/processed\")\n",
        "    if alt.parent.exists():\n",
        "        OUTPUT_DIR = alt\n",
        "\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "OUTPUT_YOUTUBE_SENTIMENT_PATH = OUTPUT_DIR / \"youtube_final_dataset_sentiment.csv\"\n",
        "\n",
        "text_col = next((c for c in [\"comment\", \"text\", \"content\", \"body\"] if c in df.columns), None)\n",
        "if text_col is None:\n",
        "    raise ValueError(\"No text column found (expected one of: comment, text, content, body).\")\n",
        "\n",
        "print(f\"üìù Using text column: {text_col}\")\n",
        "\n",
        "df[text_col] = df[text_col].fillna(\"\").astype(str).str.strip()\n",
        "df = df[~df[text_col].isin([\"[deleted]\", \"[removed]\"])].reset_index(drop=True)\n",
        "\n",
        "model_name = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    model_name,\n",
        "    model_max_length=512,\n",
        "    truncation=True,\n",
        "    padding_side=\"right\"\n",
        ")\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "sentiment_pipeline = pipeline(\n",
        "    \"text-classification\",\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device=device\n",
        ")\n",
        "\n",
        "texts = df[text_col].tolist()\n",
        "batch_size = 128\n",
        "results = []\n",
        "\n",
        "print(\"\\nüîÑ Running 3-class sentiment (pos/neu/neg) with truncation to 512 tokens...\\n\")\n",
        "for i in range(0, len(texts), batch_size):\n",
        "    batch = texts[i:i + batch_size]\n",
        "    out = sentiment_pipeline(batch, truncation=True, padding=True, max_length=512)\n",
        "    results.extend(out)\n",
        "\n",
        "    if (i // batch_size) % 10 == 0:\n",
        "        print(f\"Processed {min(i + batch_size, len(texts))}/{len(texts)}\", end=\"\\r\")\n",
        "\n",
        "    time.sleep(0.01)\n",
        "\n",
        "print(\"\\n‚úÖ Sentiment analysis complete!\")\n",
        "\n",
        "df[\"sentiment_label\"] = [r[\"label\"].upper() for r in results]\n",
        "df[\"sentiment_score\"] = [r[\"score\"] for r in results]\n",
        "\n",
        "df.to_csv(OUTPUT_YOUTUBE_SENTIMENT_PATH, index=False, encoding=\"utf-8\")\n",
        "print(f\"üíæ Saved YouTube sentiment file to:\\n{OUTPUT_YOUTUBE_SENTIMENT_PATH}\")\n",
        "\n",
        "df[[text_col, \"sentiment_label\", \"sentiment_score\"]].head(5)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# --- Find the sentiment file in common locations ---\n",
        "CANDIDATE_PATHS = [\n",
        "    Path(\"data/processed/youtube_final_dataset_sentiment.csv\"),\n",
        "    Path(\"../data/processed/youtube_final_dataset_sentiment.csv\"),\n",
        "]\n",
        "\n",
        "YOUTUBE_SENTIMENT = next((p for p in CANDIDATE_PATHS if p.exists()), None)\n",
        "if YOUTUBE_SENTIMENT is None:\n",
        "    raise FileNotFoundError(\n",
        "        \"Could not find youtube_final_dataset_sentiment.csv.\\n\"\n",
        "        \"Run the sentiment generation block first.\\n\"\n",
        "        \"Expected: data/processed/youtube_final_dataset_sentiment.csv\"\n",
        "    )\n",
        "\n",
        "df_youtube = pd.read_csv(YOUTUBE_SENTIMENT)\n",
        "print(f\"Loaded {len(df_youtube):,} YouTube rows from: {YOUTUBE_SENTIMENT}\")\n"
      ],
      "metadata": {
        "id": "QC2retzJ7QkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Nzx-YHi5nHx"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_loaded = pd.read_csv(YOUTUBE_SENTIMENT)\n",
        "text_col = next((c for c in [\"text\",\"content\",\"body\",\"comment\"] if c in df_loaded.columns), None)\n",
        "df_loaded[[text_col, \"sentiment_label\", \"sentiment_score\"]].head(100)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A5x5DIo0SnnD"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "df = pd.read_csv(YOUTUBE_SENTIMENT)\n",
        "\n",
        "text_col = next((c for c in [\"text\",\"content\",\"body\",\"comment\"] if c in df.columns), None)\n",
        "if text_col is None:\n",
        "    raise ValueError(\"No text column found in dataset.\")\n",
        "\n",
        "selected_indices = [17, 52, 55, 62, 78, 86, 97]\n",
        "\n",
        "df.loc[selected_indices, [text_col, \"sentiment_label\", \"sentiment_score\"]]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Review of Misclassficiations**"
      ],
      "metadata": {
        "id": "tH2ifjyVfePr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The comment referring to the high stream viewership suggests popularity and mass appeal, which could be interpreted as a more positive sentiment towards the brand, even though it has been classified as negative. The dataset struggles with slang, as ‚Äòate‚Äô is used as a positive description, yet classified negative. ‚ÄòMy religion‚Äô is classified as neutral, while it implies strong devotion and admiration, thus better suited as a positive sentiment. Similarly, ‚Äòbangers‚Äô is often used to reference good sounding songs, therefore better suited as positive sentiment, also explaining why the user would want to know more information about them. ‚ÄòBring back real models‚Äô is classified as neutral, yet highlights the lack thereof in reference to the show. Therefore, it creates a more negative frame of the brand. Then the final comment is framed as negative, even though the term ‚ÄòDesi ness‚Äô is a cultural reference that isn‚Äôt strictly positive or negative. Therefore, a neutral category would have suited this expression better.\n"
      ],
      "metadata": {
        "id": "1m3hFPAWe3o4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PDn1zYWS8G5"
      },
      "source": [
        "*Sentiment accuracy rate of 93%*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kNj01P3wTjIW"
      },
      "source": [
        "**3.1 Sentiment Popularity**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cIzZbGKhxtV5"
      },
      "source": [
        "This reviews the popularity of each sentiment, first by reviewing their overall presence in the dataset, the top comments of each sentiment and the average like count per category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k2ohiBphPEpf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df = pd.read_csv(YOUTUBE_SENTIMENT)\n",
        "\n",
        "counts = df[\"sentiment_label\"].value_counts()\n",
        "percentages = (counts / counts.sum() * 100).round(2)\n",
        "\n",
        "color_map = {\n",
        "    \"POSITIVE\": \"#2ECC71\",\n",
        "    \"NEGATIVE\": \"#E74C3C\",\n",
        "    \"NEUTRAL\": \"#F1C40F\"\n",
        "}\n",
        "\n",
        "bar_colors = [color_map.get(label.upper(), \"gray\") for label in counts.index]\n",
        "\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "bars = plt.bar(counts.index, counts.values, color=bar_colors)\n",
        "plt.title(\"YouTube Sentiment Distribution\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.ylabel(\"Number of Comments\")\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "for bar, pct in zip(bars, percentages):\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width()/2,\n",
        "        bar.get_height() + counts.max()*0.01,\n",
        "        f\"{pct}%\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=10\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"Sentiment distribution (% of total):\\n\")\n",
        "for label, pct in percentages.items():\n",
        "    print(f\"{label}: {pct:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LFsPq-3QJIa"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(YOUTUBE_SENTIMENT)\n",
        "df.columns = df.columns.str.lower()\n",
        "print(f\"Comments loaded: {len(df):,}\")\n",
        "\n",
        "text_col = next((c for c in [\"text\", \"content\", \"body\", \"comment\"] if c in df.columns), None)\n",
        "like_col = next((c for c in [\"likecount\", \"score\", \"likes\"] if c in df.columns), None)\n",
        "\n",
        "if text_col is None:\n",
        "    raise ValueError(\"‚ùå No text column found (expected 'text', 'content', 'body', or 'comment').\")\n",
        "if like_col is None:\n",
        "    print(\"‚ö†Ô∏è No likeCount/score column found ‚Äî defaulting likes to 0.\")\n",
        "    df[\"likecount\"] = 0\n",
        "    like_col = \"likecount\"\n",
        "\n",
        "# --- Top 15 NEGATIVE by likes ---\n",
        "most_negative_upvoted = (\n",
        "    df[df[\"sentiment_label\"].str.lower().str.contains(\"neg\")]\n",
        "    .sort_values(like_col, ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "print(\"\\nüî¥ Top 15 Most-Liked Negative Comments:\\n\")\n",
        "for _, row in most_negative_upvoted.iterrows():\n",
        "    print(f\"üí¨ {str(row[text_col])[:500]}\")\n",
        "    print(f\"   üîπ Sentiment: {row['sentiment_label']} ({row['sentiment_score']:.3f}) | Likes: {row[like_col]}\\n\")\n",
        "\n",
        "# --- Top 15 POSITIVE by likes ---\n",
        "most_positive_upvoted = (\n",
        "    df[df[\"sentiment_label\"].str.lower().str.contains(\"pos\")]\n",
        "    .sort_values(like_col, ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "print(\"\\nüü¢ Top 15 Most-Liked Positive Comments:\\n\")\n",
        "for _, row in most_positive_upvoted.iterrows():\n",
        "    print(f\"üí¨ {str(row[text_col])[:500]}\")\n",
        "    print(f\"   üîπ Sentiment: {row['sentiment_label']} ({row['sentiment_score']:.3f}) | Likes: {row[like_col]}\\n\")\n",
        "\n",
        "# --- Top 15 NEUTRAL by likes ---\n",
        "most_neutral_upvoted = (\n",
        "    df[df[\"sentiment_label\"].str.lower().str.contains(\"neu\")]\n",
        "    .sort_values(like_col, ascending=False)\n",
        "    .head(15)\n",
        ")\n",
        "\n",
        "print(\"\\nüü° Top 15 Most-Liked Neutral Comments:\\n\")\n",
        "for _, row in most_neutral_upvoted.iterrows():\n",
        "    print(f\"üí¨ {str(row[text_col])[:500]}\")\n",
        "    print(f\"   üîπ Sentiment: {row['sentiment_label']} ({row['sentiment_score']:.3f}) | Likes: {row[like_col]}\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v3jZfNq6shLP"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "df = pd.read_csv(YOUTUBE_SENTIMENT)\n",
        "df.columns = df.columns.str.lower()\n",
        "\n",
        "\n",
        "like_col = next((c for c in [\"likecount\", \"score\", \"likes\"] if c in df.columns), None)\n",
        "if like_col is None:\n",
        "    raise ValueError(\"‚ùå No like/score column found in dataset.\")\n",
        "\n",
        "\n",
        "sentiment_stats = df.groupby(\"sentiment_label\")[like_col].agg([\"mean\", \"sum\", \"count\"]).reset_index()\n",
        "sentiment_stats = sentiment_stats.sort_values(\"mean\", ascending=False)\n",
        "\n",
        "\n",
        "plt.figure(figsize=(7,4))\n",
        "\n",
        "\n",
        "color_map = {\n",
        "    \"POSITIVE\": \"#2ECC71\",\n",
        "    \"NEGATIVE\": \"#E74C3C\",\n",
        "    \"NEUTRAL\": \"#F1C40F\"\n",
        "}\n",
        "\n",
        "\n",
        "sentiment_stats['sentiment_label'] = pd.Categorical(sentiment_stats['sentiment_label'], [\"NEGATIVE\", \"NEUTRAL\", \"POSITIVE\"])\n",
        "sentiment_stats = sentiment_stats.sort_values(\"sentiment_label\")\n",
        "\n",
        "bars = plt.bar(\n",
        "    sentiment_stats[\"sentiment_label\"],\n",
        "    sentiment_stats[\"mean\"],\n",
        "    color=[color_map[label] for label in sentiment_stats[\"sentiment_label\"]]\n",
        ")\n",
        "plt.title(\"Average Likes per Sentiment Category (YouTube)\")\n",
        "plt.ylabel(\"Average Likes per Comment\")\n",
        "plt.xlabel(\"Sentiment\")\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "\n",
        "for bar in bars:\n",
        "    plt.text(\n",
        "        bar.get_x() + bar.get_width()/2,\n",
        "        bar.get_height() + sentiment_stats[\"mean\"].max()*0.01,\n",
        "        f\"{bar.get_height():.0f}\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=9\n",
        "    )\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"üìä Sentiment Popularity Summary:\")\n",
        "for _, row in sentiment_stats.iterrows():\n",
        "    print(f\"{row['sentiment_label']}: {row['count']} comments | Avg Likes: {row['mean']:.1f} | Total Likes: {row['sum']:.0f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xb9AHvdGNnO6"
      },
      "source": [
        "**4.0 Text Frequency Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHPHTyrrcHX7"
      },
      "source": [
        "This part includes an analysis of the most frequent words in the dataset. From the most frequent or thematically relevant words, the top 10 most liked comments are reviewed, to gain deeper contextual awareness."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c7b3b323"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(CLEAN_YOUTUBE_DATA)\n",
        "\n",
        "text_col = next((c for c in [\"text\",\"content\",\"body\",\"comment\"] if c in df.columns), None)\n",
        "if text_col is None:\n",
        "    raise ValueError(\"No text column found.\")\n",
        "\n",
        "all_text = \" \".join(df[text_col].dropna().astype(str).tolist())\n",
        "\n",
        "vectorizer = CountVectorizer(stop_words=\"english\", token_pattern=r\"\\b\\w+\\b\")\n",
        "X = vectorizer.fit_transform([all_text])\n",
        "\n",
        "words = vectorizer.get_feature_names_out()\n",
        "counts = X.sum(axis=0).tolist()[0]\n",
        "\n",
        "word_counts = pd.DataFrame({\"word\": words, \"count\": counts})\n",
        "\n",
        "custom_stopwords = {\"s\",\"t\",\"g\",\"1\",\"m\",\"la\",\"000\", 'y', 'don', 've', 'el', 'que', 'did'}\n",
        "word_counts = word_counts[~word_counts[\"word\"].isin(custom_stopwords)]\n",
        "\n",
        "top_words = (\n",
        "    word_counts\n",
        "    .sort_values(\"count\", ascending=False)\n",
        "    .head(15)\n",
        "    .reset_index(drop=True)\n",
        ")\n",
        "\n",
        "top_words.index = top_words.index + 1\n",
        "\n",
        "print(\"Top 15 Words in the Dataset:\")\n",
        "display(\n",
        "    top_words.style.set_properties(\n",
        "        **{\n",
        "            \"border\": \"1px solid black\",\n",
        "            \"text-align\": \"left\",\n",
        "        }\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "pd.set_option(\"display.max_rows\", 100)\n",
        "\n",
        "TOP_N_WORDS = 15\n",
        "TOP_N_COMMENTS = 15\n",
        "LIKES_COL = \"likeCount\"\n",
        "\n",
        "for word in top_words[\"word\"].head(TOP_N_WORDS):\n",
        "    subset = df[\n",
        "        df[text_col]\n",
        "        .fillna(\"\")\n",
        "        .astype(str)\n",
        "        .str.contains(rf\"\\b{word}\\b\", case=False, regex=True)\n",
        "    ]\n",
        "\n",
        "    if subset.empty:\n",
        "        continue\n",
        "\n",
        "    cols_to_show = [text_col, LIKES_COL] + ([\"orig_index\"] if \"orig_index\" in df.columns else [])\n",
        "\n",
        "    top_comments = (\n",
        "        subset.sort_values(LIKES_COL, ascending=False)\n",
        "              .head(TOP_N_COMMENTS)[cols_to_show]\n",
        "              .reset_index(drop=True)\n",
        "    )\n",
        "\n",
        "    print(f\"\\nüîπ Top {TOP_N_COMMENTS} most-liked comments containing '{word}':\\n\")\n",
        "    display(top_comments)\n"
      ],
      "metadata": {
        "id": "P_KcO8POcIMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_8_3MaP7ip4"
      },
      "source": [
        " **5.0 Topic Modelling**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ElDJPB0d03_"
      },
      "source": [
        "This part conducts topic modelling on the entire dataset using semantic embeddings, which capture the meaning of the word and don‚Äôt rely exclusively on frequency patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yR6kgBeosyBk"
      },
      "outputs": [],
      "source": [
        "!pip install -q bertopic sentence-transformers umap-learn hdbscan nltk\n",
        "\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from bertopic import BERTopic\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "df_yt = pd.read_csv(CLEAN_YOUTUBE_DATA)\n",
        "print(f\"‚úÖ Loaded cleaned YouTube dataset with {len(df_yt):,} rows\")\n",
        "\n",
        "text_col = next((c for c in [\"comment\", \"text\", \"content\", \"body\"] if c in df_yt.columns), None)\n",
        "if text_col is None:\n",
        "    raise ValueError(\"‚ùå No text column found (expected one of: comment, text, content, body).\")\n",
        "\n",
        "df_yt[text_col] = df_yt[text_col].fillna(\"\").astype(str)\n",
        "\n",
        "\n",
        "df_yt = df_yt[df_yt[text_col].str.strip().ne(\"\")].copy()\n",
        "print(f\"üßπ Kept all {len(df_yt):,} comments\")\n",
        "\n",
        "df_yt[\"char_len\"] = df_yt[text_col].str.len()\n",
        "df_yt[\"is_short\"] = df_yt[\"char_len\"] < 5\n",
        "\n",
        "\n",
        "# Stopwords\n",
        "nltk.download(\"stopwords\")\n",
        "\n",
        "stopwords_multi = (\n",
        "    set(stopwords.words(\"english\"))\n",
        "    | set(stopwords.words(\"spanish\"))\n",
        "    | set(stopwords.words(\"portuguese\"))\n",
        "    | {\n",
        "        \"https\", \"http\", \"www\", \"com\", \"jpg\", \"removed\", \"deleted\", \"amp\",\n",
        "        \"vs\", \"victoria\", \"secret\", \"victorias\", \"show\",\n",
        "        \"like\", \"really\", \"girl\", \"girls\", \"omg\", \"lol\",\n",
        "        \"video\", \"watching\", \"watched\", \"youtube\"\n",
        "    }\n",
        ")\n",
        "stopwords_multi = list(stopwords_multi)\n",
        "\n",
        "\n",
        "vectorizer_model = CountVectorizer(\n",
        "    stop_words=stopwords_multi,\n",
        "    token_pattern=r\"[A-Za-z\\u00C0-\\u00FF']{3,}\",\n",
        "    min_df=2,\n",
        "    max_df=0.9,\n",
        "    ngram_range=(1, 2)\n",
        ")\n",
        "\n",
        "\n",
        "embedding_model = SentenceTransformer(\"paraphrase-multilingual-mpnet-base-v2\")\n",
        "\n",
        "topic_model = BERTopic(\n",
        "    embedding_model=embedding_model,\n",
        "    vectorizer_model=vectorizer_model,\n",
        "    language=\"multilingual\",\n",
        "    min_topic_size=50,\n",
        "    calculate_probabilities=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Fit model\n",
        "texts = df_yt[text_col].tolist()\n",
        "topics, probs = topic_model.fit_transform(texts)\n",
        "df_yt[\"bertopic_topic\"] = topics\n",
        "\n",
        "\n",
        "# Output topic overview\n",
        "pd.set_option(\"display.max_colwidth\", None)\n",
        "topic_info = topic_model.get_topic_info()\n",
        "\n",
        "print(f\"\\nüìä Topic overview: total {len(topic_info)} topics (including -1 outliers)\")\n",
        "print(\"\\n‚úÖ Sanity check (all rows assigned a topic):\")\n",
        "print(\"Rows in df_yt:\", len(df_yt))\n",
        "print(\"Topic assignments:\", df_yt[\"bertopic_topic\"].notna().sum())\n",
        "print(\"\\nTopic distribution:\")\n",
        "print(df_yt[\"bertopic_topic\"].value_counts().sort_index())\n",
        "\n",
        "top_topic_info = topic_info[topic_info[\"Topic\"] != -1].head(15)\n",
        "print(\"\\nüìä Top 15 non-outlier topics:\")\n",
        "print(top_topic_info.to_string(index=False))\n",
        "\n",
        "top_topics = top_topic_info[\"Topic\"].tolist()\n",
        "\n",
        "print(\"\\nüîç Inspecting TOP 15 topics (excluding -1) with 15 example comments each:\")\n",
        "for t in top_topics:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"üß© Topic {t}\")\n",
        "    print(\"Top words:\", topic_model.get_topic(t))\n",
        "    print(\"\\nüí¨ Sample comments:\")\n",
        "\n",
        "    sample_comments = df_yt.loc[df_yt[\"bertopic_topic\"] == t, text_col].head(15)\n",
        "    for i, c in enumerate(sample_comments, start=1):\n",
        "        print(f\"{i:2d}. {c}\")\n",
        "\n",
        "\n",
        "outlier_mask = df_yt[\"bertopic_topic\"] == -1\n",
        "outlier_count = int(outlier_mask.sum())\n",
        "print(\"\\nüìå Outlier diagnostics:\")\n",
        "print(f\"Outlier (-1) comments: {outlier_count:,} / {len(df_yt):,} ({outlier_count/len(df_yt)*100:.1f}%)\")\n",
        "\n",
        "if outlier_count > 0:\n",
        "    short_outliers = int(df_yt.loc[outlier_mask, \"is_short\"].sum())\n",
        "    print(f\"Short (<5 chars) within outliers: {short_outliers:,} / {outlier_count:,} \"\n",
        "          f\"({short_outliers/outlier_count*100:.1f}%)\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}